# missing
## Allegheny Family Screening Tool (AFST)
- Algorithm for screening for child welfare investigations in Allegheny County (Pittsburgh) used by Office for Children, Youth, and Families (CYF)

## What are the takeaways?
At first, Eubanks seems to confirm aspects of the standard narrative
- the algorithm in unfair because its not very accurate
- The algorithm is unfair because it is trained on limimted data
- The algorithm is unfair because it uses biased proxies
But her story also helps us

## Fairness beyond accuracy: procedural rights
Fairness as due process (USA: Fifth and Fourteenth Amendments)

Predictive policing and fourth Amendment protections against
unreasonable search and seizure

## Beyond Fairness: Representation ([[Lecture 1.4 - Sept5]])
Being classified as risky in a database stigmatizes

Circulation and durability: tags in one database can follow you through other databases, and can be difficult or impossible to fix or remove

## Beyond Fairness: Performativity of predictions
“Self-fulfilling prophecy”
Performativity or risk classification through feedback loops

## Beyond Fairness: Agency
- predicitive systems restrict or foreclose agency

## Agency
Agency is the ability of an individual or collective to act.  
Contexts always shape the spaces for action and who has agency.

## How does ADM shape agency?
Who is responsible when things break down?

## How does ADM shape agency ⤱ and institutions?  
Does a “human-in-the-loop” fix algorithmic bias?  
How do risk assessment systems shape the the prominence of risk in human decision-making?  
How does it shape the goals of the institution?
risk-assessment gets institutionalized as a key category of justice and good governance

## What does the institution using ADM actually do?  
In the case of the Allegheny Office for Children, Youth, and Families (and most child welfare agencies):  
- Provide welfare, material support  
- Remove children from homes, place them in foster care  
Use of public resources indicates poverty, which increases risk score, and therefore the proximity to criminality/child abuse

## Is ADM “inherently political?”
Automated decision-making shatters the social safety net, criminalizes the poor, intensifies discrimination, and compromises our deepest national values. It reframes shared social decisions about who we are and who we want to be into systems engineering problems.” (Eubanks, 12)

## What does automated decision-making do?
ADM is a technique for making social order through:
Observation/Surveillance  
Identification/Tracking  
Classification/Sorting  
Prediction/Risk  
Ranking/Norming  
Deciding/Acting

## Bentham’s Panopticon (1791)
(Self-)Discipline through surveillance  
- Architecture for constant, total visibility  
- Hierarchical observation  
- Normalizing judgment
Panopticon as speculative template for social power in the 19th and early 20th century  
- Michel Foucault, Discipline and Punish (1975)

# missing

## Panoptic Social Sorting in the Age of Social Media
“Social credit score?”  
Generalized panopticism in the algorithmic culture of social media
### Summary: Automated Decision-Making and "The Allegheny Algorithm"

This lecture builds on the idea that technology is political by examining Automated Decision-Making (ADM) and risk assessment systems. The core argument is that evaluating these systems solely on the basis of "fairness-as-accuracy" is insufficient. Instead, one must consider a broader range of ethically relevant dimensions and view ADM as a powerful technique for governance that creates social order.

**Core Case Study: The Allegheny Family Screening Tool (AFST)**

The primary case study is the "Allegheny Algorithm," an ADM tool used by the Office for Children, Youth, and Families (CYF) in Allegheny County, Pennsylvania, to screen for child welfare investigations. The AFST predicts a child's risk score, which influences decisions made by the agency. While the system's fairness is often questioned based on standard critiques—such as its limited accuracy, reliance on biased data, and use of poor proxies for "ground truth"—the lecture uses this case to explore deeper ethical issues.

**Ethical Dimensions Beyond Fairness-as-Accuracy**

The lecture highlights several critical dimensions of ADM systems that are not captured by a simple analysis of their accuracy:

- **Procedural Rights:** Predictive systems can conflict with fundamental legal protections. For instance, predictive policing may clash with the Fourth Amendment's protection against unreasonable search and seizure, and other ADM systems can violate due process rights guaranteed by the Fifth and Fourteenth Amendments.
- **Representation:** Being classified as "risky" by a database carries a social stigma. These classifications can be durable and follow individuals across different systems, creating a permanent digital record that is difficult to alter or remove.
- **Performativity:** Predictions made by ADM systems can become self-fulfilling prophecies. For example, if a system predicts an individual is at high risk of a negative outcome, that person may be subjected to increased scrutiny or denied resources, which in turn makes the negative outcome more likely, creating a feedback loop.
- **Agency:** ADM systems can restrict or foreclose human agency—the ability of individuals or groups to act. They can also obscure responsibility when things go wrong, creating "moral crumple zones" where it is unclear who is accountable for a system's failure. The lecture questions whether simply having a "human-in-the-loop" is an adequate solution to algorithmic bias.
- **Institutional Transformation:** The use of ADM can fundamentally change the goals and logic of the institutions that adopt them. In the case of the Allegheny CYF, the focus shifts from providing welfare and material support to managing risk. This reframes social issues like poverty, which becomes a proxy for risk, thereby criminalizing the poor and transforming shared social decisions into "systems engineering problems".

**ADM as a Form of Governance: The Panopticon**

To understand the broader political function of ADM, the lecture presents it as a modern form of governance that creates social order through surveillance, sorting, and discipline. This is conceptualized using the historical template of Jeremy Bentham's **Panopticon**—an architectural design for a prison that allows for constant, hierarchical observation, leading to self-discipline among the observed.

ADM operates as a "panoptic" technology by creating social order through:

- Observation and Surveillance
- Identification and Tracking
- Classification and Sorting
- Prediction and Risk Assessment
- Ranking and Normalizing Judgment

This logic of panoptic social sorting is not confined to public welfare agencies but was pioneered in the private sector through insurance and credit scoring and is now widely used in areas like employment screening, loan applications, and university admissions.

---

**Suggested Next Step:** The lecture provides a set of diagnostic questions related to the concept of "Agency". As a next step, we could use these specific questions to analyze how agency is shaped, afforded, or denied for different actors (e.g., social workers, families, the algorithm itself) within the Allegheny Family Screening Tool case study. This would allow us to move from a high-level summary to a more granular analysis of a key ethical dimension.